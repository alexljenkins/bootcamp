{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "### Understand the theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Practical achievements in the field\n",
    "* Supervised / Unsupervised / Reinforcement\n",
    "* Pavlov to Bellman\n",
    "* Environment / State / Action / Reward\n",
    "* Drawbacks - curse of dimensionality, credit assignment problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement it in practice using OpenAI's Gym\n",
    "* A handy library for learning about RL - https://gym.openai.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install gym`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's work on the cartpole problem\n",
    "#### First we make an environment in which the agent can be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we implement the agent-environment loop\n",
    "* Start the process by resetting the environment\n",
    "* And return an initial observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02989791,  0.0465321 ,  0.01970152, -0.04242339])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_obs = env.reset()\n",
    "initial_obs #position, speed, angle of pole, rotation of pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve the same thing by taking an action - in this case a  `step` in a given direction, 0 for left and 1 for right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, _ = env.step(0) #0 = move left, 1 = move right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already use the `done` boolean to work out if we can stop the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use `sample` the `action_space` space to randomly pick an action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_step = env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `render` the environment to see what our cart is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n",
    "time.sleep(5)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK, but we need to build an RL agent. What next?**\n",
    "\n",
    "First, lets try to build the simplest RL agent:\n",
    "* If the pole is left, move left\n",
    "* If the pole is right, move right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rl(env):\n",
    "    # reset env and take a step\n",
    "    obs = env.reset()\n",
    "    # loop over:\n",
    "    for i in range(1000):\n",
    "        # measure: is pole angled left or right?\n",
    "        # action: if left -> move left, if right -> move right\n",
    "        if obs[2] < 0:\n",
    "            action = 0\n",
    "        elif obs[2] > 0:\n",
    "            action = 1\n",
    "        else:\n",
    "            break\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        time.sleep(0.1)\n",
    "        env.render()\n",
    "        if done:\n",
    "            print(f'iterations survived: {i}')\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations survived: 40\n"
     ]
    }
   ],
   "source": [
    "simple_rl(env) #base model 36-45 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I think we can do better than that. Lets build a model which learns to move better based on training data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First we need to generate some training data\n",
    "* X = obs\n",
    "* y = done bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_training_data(env):\n",
    "    # create 1000 virtual games\n",
    "    number_of_games = 10000\n",
    "    last_moves = 20\n",
    "    observations = []\n",
    "    actions = []\n",
    "    \n",
    "    for i in range(number_of_games):\n",
    "        #in each game\n",
    "        game_observations = []\n",
    "        game_actions = []\n",
    "        obs = env.reset()\n",
    "        \n",
    "        for j in range(1000):\n",
    "            # take a series of random steps\n",
    "            action = env.action_space.sample()\n",
    "            #measure how that action changed the state\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            # store results\n",
    "            game_observations.append(obs)\n",
    "            game_actions.append(action)\n",
    "            \n",
    "            if done: #if the agent dies\n",
    "                #record everything except the end which is rubbish data\n",
    "                observations += game_observations[:-last_moves]\n",
    "                actions += game_actions[1:-(last_moves-1)]\n",
    "                break\n",
    "                \n",
    "    return np.array(observations), np.array(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then a model which plays based on its predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_rl(env, m):\n",
    "    # reset env and take a step\n",
    "    obs = env.reset()\n",
    "    # loop over:\n",
    "    for i in range(700):\n",
    "        # m.predict model's next best move\n",
    "        obs = obs.reshape(-1,4)\n",
    "        action = int(m.predict(obs))\n",
    "        #take model's idea of the right move\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        time.sleep(0.1)\n",
    "        env.render()\n",
    "        if done:\n",
    "            print(f'iterations survived: {i}')\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets run the code, and measure the improvement\n",
    "* Setup the gym\n",
    "* Collect training data\n",
    "* Train a model\n",
    "* And play\n",
    "* And measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = collect_training_data(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "m = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexl\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations survived: 226\n"
     ]
    }
   ],
   "source": [
    "smart_rl(env,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexl\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations survived: 499\n"
     ]
    }
   ],
   "source": [
    "smart_rl(env,lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
